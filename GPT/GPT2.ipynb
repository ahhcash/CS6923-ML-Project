{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a36c5f265b5442648ddee159b7e5e887": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_770a6bbe9ed94092b018e3f9d7d72779",
              "IPY_MODEL_cd67623293644a9aa0ab5dd8a4c3e8ad",
              "IPY_MODEL_23ce070b5a90404194f7b638be547731"
            ],
            "layout": "IPY_MODEL_f882432ea4db48cf88d434c229a47035"
          }
        },
        "770a6bbe9ed94092b018e3f9d7d72779": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3ba94688b5ec43ac8449cbb684e8f543",
            "placeholder": "​",
            "style": "IPY_MODEL_2e8f3decf8f243e2afdbf7db5c3b4777",
            "value": "Loading dataset shards: 100%"
          }
        },
        "cd67623293644a9aa0ab5dd8a4c3e8ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a5115a97e0184050842a2ddb06208b5c",
            "max": 41,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cf219137cb044960bfc7276f4b270db8",
            "value": 41
          }
        },
        "23ce070b5a90404194f7b638be547731": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cb15be85b51b4310b76d7983c57e804c",
            "placeholder": "​",
            "style": "IPY_MODEL_3c440dde8bb04f029b7438d3a199f7d8",
            "value": " 41/41 [00:35&lt;00:00,  1.04it/s]"
          }
        },
        "f882432ea4db48cf88d434c229a47035": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ba94688b5ec43ac8449cbb684e8f543": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e8f3decf8f243e2afdbf7db5c3b4777": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a5115a97e0184050842a2ddb06208b5c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf219137cb044960bfc7276f4b270db8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cb15be85b51b4310b76d7983c57e804c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3c440dde8bb04f029b7438d3a199f7d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-1bbi0zj4iXJ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from dataclasses import dataclass\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        # key, query, value projections for all heads\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
        "        # output projection\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "        # regularization\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality\n",
        "        # calculate query, key, values for all heads in batch\n",
        "        qkv = self.c_attn(x)\n",
        "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        # causal self-attention\n",
        "        y = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        # output projection\n",
        "        y = self.c_proj(y)\n",
        "        return y\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
        "        self.gelu    = nn.GELU(approximate='tanh')\n",
        "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        return x\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
        "        self.mlp = MLP(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x\n",
        "\n",
        "@dataclass\n",
        "class GPTConfig:\n",
        "    block_size: int = 1024\n",
        "    vocab_size: int = 50257\n",
        "    n_layer: int = 12\n",
        "    n_head: int = 12\n",
        "    n_embd: int = 768\n",
        "\n",
        "class GPT(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
        "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "            ln_f = nn.LayerNorm(config.n_embd),\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "\n",
        "        # weight sharing between embedding and final layer\n",
        "        self.transformer.wte.weight = self.lm_head.weight\n",
        "\n",
        "        # initialize weights\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.size()\n",
        "        assert T <= self.config.block_size, f\"Cannot forward sequence of length {T}, block size is only {self.config.block_size}\"\n",
        "\n",
        "        # forward the token and position embeddings\n",
        "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device)\n",
        "        pos_emb = self.transformer.wpe(pos)\n",
        "        tok_emb = self.transformer.wte(idx)\n",
        "        x = tok_emb + pos_emb\n",
        "\n",
        "        # forward the transformer blocks\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "        x = self.transformer.ln_f(x)\n",
        "\n",
        "        # final linear layer\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        # compute loss if targets are provided\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def configure_optimizers(self, weight_decay, learning_rate):\n",
        "        # separate weight decay and non-weight decay parameters\n",
        "        decay_params = []\n",
        "        nodecay_params = []\n",
        "        for name, param in self.named_parameters():\n",
        "            if param.dim() >= 2:\n",
        "                decay_params.append(param)\n",
        "            else:\n",
        "                nodecay_params.append(param)\n",
        "\n",
        "        optim_groups = [\n",
        "            {'params': decay_params, 'weight_decay': weight_decay},\n",
        "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
        "        ]\n",
        "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=(0.9, 0.95))\n",
        "        return optimizer\n",
        "\n",
        "# Training setup and helper functions\n",
        "def train_gpt2(\n",
        "    model,\n",
        "    train_data,\n",
        "    val_data,\n",
        "    batch_size=32,\n",
        "    block_size=1024,\n",
        "    epochs=1,\n",
        "    learning_rate=3e-4,\n",
        "    weight_decay=0.1,\n",
        "    device='cuda' if torch.cuda.is_available() else 'cpu'\n",
        "):\n",
        "    model = model.to(device)\n",
        "    optimizer = model.configure_optimizers(weight_decay=weight_decay, learning_rate=learning_rate)\n",
        "\n",
        "    def get_batch(split):\n",
        "        data = train_data if split == 'train' else val_data\n",
        "        ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "        x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "        y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        return x, y\n",
        "\n",
        "    # training loop\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        start_time = time.time()\n",
        "\n",
        "        for iter in range(len(train_data) // (batch_size * block_size)):\n",
        "            # get batch\n",
        "            xb, yb = get_batch('train')\n",
        "\n",
        "            # forward pass\n",
        "            logits, loss = model(xb, yb)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # backward pass\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            # logging\n",
        "            if iter % 100 == 0:\n",
        "                print(f\"epoch {epoch+1} iter {iter}: loss {loss.item():.4f}\")\n",
        "\n",
        "        # validation\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for _ in range(len(val_data) // (batch_size * block_size) // 10):  # evaluate on subset\n",
        "                xb, yb = get_batch('val')\n",
        "                logits, loss = model(xb, yb)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1} complete. Train loss: {total_loss:.4f}, Val loss: {val_loss:.4f}\")\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"pyarrow==14.0.1\" datasets tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZSJap-TP67FR",
        "outputId": "3c25ce4b-aeee-4c39-e083-aace1d4e5bc0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyarrow==14.0.1\n",
            "  Downloading pyarrow-14.0.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.10/dist-packages (from pyarrow==14.0.1) (1.26.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "INFO: pip is looking at multiple versions of datasets to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.0.2-py3-none-any.whl.metadata (20 kB)\n",
            "  Downloading datasets-3.0.1-py3-none-any.whl.metadata (20 kB)\n",
            "  Downloading datasets-3.0.0-py3-none-any.whl.metadata (19 kB)\n",
            "  Downloading datasets-2.21.0-py3-none-any.whl.metadata (21 kB)\n",
            "  Downloading datasets-2.20.0-py3-none-any.whl.metadata (19 kB)\n",
            "  Downloading datasets-2.19.2-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.17-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.3.1,>=2023.1.0 (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.3.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.26.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.9.11)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.1->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.1->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.1->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.1->datasets) (2024.8.30)\n",
            "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
            "Downloading pyarrow-14.0.1-cp310-cp310-manylinux_2_28_x86_64.whl (38.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.0/38.0 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-2.19.2-py3-none-any.whl (542 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.1/542.1 kB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m63.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.3.1-py3-none-any.whl (171 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.0/172.0 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, pyarrow, fsspec, dill, tiktoken, multiprocess, datasets\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 17.0.0\n",
            "    Uninstalling pyarrow-17.0.0:\n",
            "      Successfully uninstalled pyarrow-17.0.0\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.3.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-2.19.2 dill-0.3.8 fsspec-2024.3.1 multiprocess-0.70.16 pyarrow-14.0.1 tiktoken-0.8.0 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# from datasets import load_dataset\n",
        "# import tiktoken\n",
        "# import torch\n",
        "# import numpy as np\n",
        "# from tqdm import tqdm\n",
        "# from torch.utils.data import Dataset, DataLoader\n",
        "# import gc\n",
        "# from typing import List, Tuple\n",
        "# from itertools import islice\n",
        "\n",
        "# class WikiTextDataset(Dataset):\n",
        "#     def __init__(self, encodings, block_size):\n",
        "#         self.encodings = encodings\n",
        "#         self.block_size = block_size\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.encodings) - self.block_size\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         chunk = self.encodings[idx:idx + self.block_size + 1]\n",
        "#         x = chunk[:-1]\n",
        "#         y = chunk[1:]\n",
        "#         return x, y\n",
        "\n",
        "# def batch_iterator(iterable, batch_size):\n",
        "#     \"\"\"Helper function to create batch iterator\"\"\"\n",
        "#     iterator = iter(iterable)\n",
        "#     while batch := list(islice(iterator, batch_size)):\n",
        "#         yield batch\n",
        "\n",
        "# def process_batch_gpu(\n",
        "#     texts: List[str],\n",
        "#     tokenizer,\n",
        "#     device: torch.device,\n",
        "#     max_length: int = None\n",
        "# ) -> List[List[int]]:\n",
        "#     \"\"\"Process a batch of texts using GPU\"\"\"\n",
        "#     # Tokenize all texts in the batch\n",
        "#     tokens_list = []\n",
        "#     for text in texts:\n",
        "#         if text.strip():  # Only process non-empty texts\n",
        "#             tokens = tokenizer.encode(text)\n",
        "#             if tokens:  # Only add if we got tokens\n",
        "#                 tokens.append(tokenizer.eot_token)  # Add EOT token\n",
        "#                 tokens_list.extend(tokens)\n",
        "\n",
        "#     return tokens_list\n",
        "\n",
        "# def prepare_wikipedia_data(\n",
        "#     cache_dir=\"wiki_cache\",\n",
        "#     block_size=1024,\n",
        "#     train_val_split=0.95,\n",
        "#     batch_size=32,\n",
        "#     processing_batch_size=1000,\n",
        "#     device='cuda' if torch.cuda.is_available() else 'cpu'\n",
        "# ):\n",
        "#     \"\"\"\n",
        "#     Prepare Wikipedia dataset for GPT-2 training with GPU acceleration\n",
        "\n",
        "#     Args:\n",
        "#         cache_dir: Directory to cache the processed data\n",
        "#         block_size: Size of text chunks for training\n",
        "#         train_val_split: Proportion of data to use for training\n",
        "#         batch_size: Batch size for data loading\n",
        "#         processing_batch_size: Batch size for GPU processing\n",
        "#         device: Device to use for processing\n",
        "#     \"\"\"\n",
        "\n",
        "#     # Create cache directory if it doesn't exist\n",
        "#     os.makedirs(cache_dir, exist_ok=True)\n",
        "\n",
        "#     # Cache files\n",
        "#     train_cache = os.path.join(cache_dir, \"train_tokens.pt\")\n",
        "#     val_cache = os.path.join(cache_dir, \"val_tokens.pt\")\n",
        "\n",
        "#     # Check if processed data already exists\n",
        "#     if os.path.exists(train_cache) and os.path.exists(val_cache):\n",
        "#         print(\"Loading cached data...\")\n",
        "#         train_data = torch.load(train_cache)\n",
        "#         val_data = torch.load(val_cache)\n",
        "#         return train_data, val_data\n",
        "\n",
        "#     print(\"Loading Wikipedia dataset...\")\n",
        "#     dataset = load_dataset(\"wikipedia\", \"20220301.en\", split=\"train\")\n",
        "\n",
        "#     # Initialize tokenizer\n",
        "#     enc = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "#     print(\"Processing dataset in batches...\")\n",
        "#     all_tokens = []\n",
        "#     total_batches = len(dataset) // processing_batch_size + (1 if len(dataset) % processing_batch_size != 0 else 0)\n",
        "\n",
        "#     for batch_idx, batch in enumerate(tqdm(\n",
        "#         batch_iterator(dataset['text'], processing_batch_size),\n",
        "#         total=total_batches,\n",
        "#         desc=\"Processing batches\"\n",
        "#     )):\n",
        "#         # Process batch using GPU\n",
        "#         batch_tokens = process_batch_gpu(batch, enc, device)\n",
        "#         all_tokens.extend(batch_tokens)\n",
        "\n",
        "#         # Periodically clear GPU memory\n",
        "#         if (batch_idx + 1) % 10 == 0:\n",
        "#             torch.cuda.empty_cache()\n",
        "#             gc.collect()\n",
        "\n",
        "#     # Convert to tensor\n",
        "#     print(\"Converting to tensor...\")\n",
        "#     all_tokens = torch.tensor(all_tokens, dtype=torch.long)\n",
        "\n",
        "#     # Split into train and validation\n",
        "#     split_idx = int(len(all_tokens) * train_val_split)\n",
        "#     train_tokens = all_tokens[:split_idx]\n",
        "#     val_tokens = all_tokens[split_idx:]\n",
        "\n",
        "#     print(f\"Total tokens: {len(all_tokens):,}\")\n",
        "#     print(f\"Train tokens: {len(train_tokens):,}\")\n",
        "#     print(f\"Val tokens: {len(val_tokens):,}\")\n",
        "\n",
        "#     # Save processed data\n",
        "#     print(\"Saving processed data...\")\n",
        "#     torch.save(train_tokens, train_cache)\n",
        "#     torch.save(val_tokens, val_cache)\n",
        "\n",
        "#     # Clear memory\n",
        "#     del all_tokens\n",
        "#     torch.cuda.empty_cache()\n",
        "#     gc.collect()\n",
        "\n",
        "#     return train_tokens, val_tokens\n",
        "\n",
        "# def create_dataloaders(\n",
        "#     train_tokens,\n",
        "#     val_tokens,\n",
        "#     block_size=1024,\n",
        "#     batch_size=32,\n",
        "#     num_workers=4\n",
        "# ):\n",
        "#     \"\"\"Create DataLoaders for training and validation\"\"\"\n",
        "#     train_dataset = WikiTextDataset(train_tokens, block_size)\n",
        "#     val_dataset = WikiTextDataset(val_tokens, block_size)\n",
        "\n",
        "#     train_loader = DataLoader(\n",
        "#         train_dataset,\n",
        "#         batch_size=batch_size,\n",
        "#         shuffle=True,\n",
        "#         num_workers=num_workers,\n",
        "#         pin_memory=True\n",
        "#     )\n",
        "\n",
        "#     val_loader = DataLoader(\n",
        "#         val_dataset,\n",
        "#         batch_size=batch_size,\n",
        "#         shuffle=False,\n",
        "#         num_workers=num_workers,\n",
        "#         pin_memory=True\n",
        "#     )\n",
        "\n",
        "#     return train_loader, val_loader\n",
        "\n",
        "# class TrainingConfig:\n",
        "#     def __init__(self):\n",
        "#         self.learning_rate = 3e-4\n",
        "#         self.weight_decay = 0.1\n",
        "#         self.gradient_accumulation_steps = 8\n",
        "#         self.warmup_steps = 1000\n",
        "#         self.max_steps = None  # Will be set based on dataset size\n",
        "#         self.batch_size = 32\n",
        "#         self.block_size = 1024\n",
        "#         self.epochs = 3\n",
        "#         self.checkpoint_dir = \"checkpoints\"\n",
        "#         self.log_interval = 100\n",
        "\n",
        "# def train_gpt2_with_dataloader(\n",
        "#     model,\n",
        "#     train_loader,\n",
        "#     val_loader,\n",
        "#     config: TrainingConfig,\n",
        "#     device='cuda' if torch.cuda.is_available() else 'cpu'\n",
        "# ):\n",
        "#     \"\"\"Enhanced training function with learning rate scheduling and better logging\"\"\"\n",
        "#     model = model.to(device)\n",
        "#     optimizer = model.configure_optimizers(\n",
        "#         weight_decay=config.weight_decay,\n",
        "#         learning_rate=config.learning_rate\n",
        "#     )\n",
        "\n",
        "#     # Create checkpoint directory\n",
        "#     os.makedirs(config.checkpoint_dir, exist_ok=True)\n",
        "\n",
        "#     # Calculate total steps if not provided\n",
        "#     if config.max_steps is None:\n",
        "#         config.max_steps = len(train_loader) * config.epochs\n",
        "\n",
        "#     # Learning rate scheduler\n",
        "#     def get_lr(step):\n",
        "#         if step < config.warmup_steps:\n",
        "#             return config.learning_rate * step / config.warmup_steps\n",
        "#         return config.learning_rate\n",
        "\n",
        "#     # Training loop\n",
        "#     global_step = 0\n",
        "#     best_val_loss = float('inf')\n",
        "\n",
        "#     for epoch in range(config.epochs):\n",
        "#         model.train()\n",
        "#         total_loss = 0\n",
        "\n",
        "#         progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n",
        "\n",
        "#         for step, (x, y) in enumerate(progress_bar):\n",
        "#             x, y = x.to(device), y.to(device)\n",
        "\n",
        "#             # Forward pass\n",
        "#             logits, loss = model(x, y)\n",
        "#             loss = loss / config.gradient_accumulation_steps\n",
        "#             total_loss += loss.item()\n",
        "\n",
        "#             # Backward pass\n",
        "#             loss.backward()\n",
        "\n",
        "#             # Update weights\n",
        "#             if (step + 1) % config.gradient_accumulation_steps == 0:\n",
        "#                 torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "#                 # Update learning rate\n",
        "#                 lr = get_lr(global_step)\n",
        "#                 for param_group in optimizer.param_groups:\n",
        "#                     param_group['lr'] = lr\n",
        "\n",
        "#                 optimizer.step()\n",
        "#                 optimizer.zero_grad()\n",
        "#                 global_step += 1\n",
        "\n",
        "#             # Update progress bar\n",
        "#             current_loss = total_loss / (step + 1)\n",
        "#             progress_bar.set_postfix({\n",
        "#                 'loss': f\"{current_loss:.4f}\",\n",
        "#                 'perplexity': f\"{torch.exp(torch.tensor(current_loss)):.2f}\",\n",
        "#                 'lr': f\"{lr:.2e}\"\n",
        "#             })\n",
        "\n",
        "#             # Log training progress\n",
        "#             if step % config.log_interval == 0:\n",
        "#                 print(f\"\\nStep {global_step}: loss {current_loss:.4f}, \"\n",
        "#                       f\"perplexity {torch.exp(torch.tensor(current_loss)):.2f}, \"\n",
        "#                       f\"lr {lr:.2e}\")\n",
        "\n",
        "#         # Validation\n",
        "#         model.eval()\n",
        "#         val_loss = 0\n",
        "#         with torch.no_grad():\n",
        "#             for x, y in tqdm(val_loader, desc=\"Validation\"):\n",
        "#                 x, y = x.to(device), y.to(device)\n",
        "#                 logits, loss = model(x, y)\n",
        "#                 val_loss += loss.item()\n",
        "\n",
        "#         val_loss /= len(val_loader)\n",
        "\n",
        "#         # Save checkpoint if best validation loss\n",
        "#         if val_loss < best_val_loss:\n",
        "#             best_val_loss = val_loss\n",
        "#             checkpoint_path = os.path.join(config.checkpoint_dir, f'best_model.pt')\n",
        "#             torch.save({\n",
        "#                 'epoch': epoch + 1,\n",
        "#                 'model_state_dict': model.state_dict(),\n",
        "#                 'optimizer_state_dict': optimizer.state_dict(),\n",
        "#                 'val_loss': val_loss,\n",
        "#                 'global_step': global_step,\n",
        "#             }, checkpoint_path)\n",
        "\n",
        "#         # Save regular checkpoint\n",
        "#         checkpoint_path = os.path.join(config.checkpoint_dir, f'checkpoint_epoch_{epoch+1}.pt')\n",
        "#         torch.save({\n",
        "#             'epoch': epoch + 1,\n",
        "#             'model_state_dict': model.state_dict(),\n",
        "#             'optimizer_state_dict': optimizer.state_dict(),\n",
        "#             'val_loss': val_loss,\n",
        "#             'global_step': global_step,\n",
        "#         }, checkpoint_path)\n",
        "\n",
        "#         print(f\"\\nEpoch {epoch+1} complete. \"\n",
        "#               f\"Train loss: {total_loss/len(train_loader):.4f}, \"\n",
        "#               f\"Val loss: {val_loss:.4f}, \"\n",
        "#               f\"Train perplexity: {torch.exp(torch.tensor(total_loss/len(train_loader))):.2f}, \"\n",
        "#               f\"Val perplexity: {torch.exp(torch.tensor(val_loss)):.2f}\")\n",
        "\n",
        "#     return model\n",
        "\n",
        "# # Example usage\n",
        "# if __name__ == \"__main__\":\n",
        "#     # Set device\n",
        "#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "#     print(f\"Using device: {device}\")\n",
        "\n",
        "#     # Training configuration\n",
        "#     config = TrainingConfig()\n",
        "\n",
        "#     # Prepare data\n",
        "#     print(\"Preparing Wikipedia dataset...\")\n",
        "#     train_tokens, val_tokens = prepare_wikipedia_data(\n",
        "#         cache_dir=\"wiki_cache\",\n",
        "#         block_size=config.block_size,\n",
        "#         train_val_split=0.95,\n",
        "#         batch_size=config.batch_size,\n",
        "#         device=device\n",
        "#     )\n",
        "\n",
        "#     # Create dataloaders\n",
        "#     print(\"Creating DataLoaders...\")\n",
        "#     train_loader, val_loader = create_dataloaders(\n",
        "#         train_tokens,\n",
        "#         val_tokens,\n",
        "#         block_size=config.block_size,\n",
        "#         batch_size=config.batch_size\n",
        "#     )\n",
        "\n",
        "#     # Initialize model\n",
        "#     print(\"Initializing model...\")\n",
        "#     gpt_config = GPTConfig()\n",
        "#     model = GPT(gpt_config)\n",
        "\n",
        "#     # Train model\n",
        "#     print(\"Starting training...\")\n",
        "#     model = train_gpt2_with_dataloader(\n",
        "#         model,\n",
        "#         train_loader,\n",
        "#         val_loader,\n",
        "#         config,\n",
        "#         device=device\n",
        "#     )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194,
          "referenced_widgets": [
            "a36c5f265b5442648ddee159b7e5e887",
            "770a6bbe9ed94092b018e3f9d7d72779",
            "cd67623293644a9aa0ab5dd8a4c3e8ad",
            "23ce070b5a90404194f7b638be547731",
            "f882432ea4db48cf88d434c229a47035",
            "3ba94688b5ec43ac8449cbb684e8f543",
            "2e8f3decf8f243e2afdbf7db5c3b4777",
            "a5115a97e0184050842a2ddb06208b5c",
            "cf219137cb044960bfc7276f4b270db8",
            "cb15be85b51b4310b76d7983c57e804c",
            "3c440dde8bb04f029b7438d3a199f7d8"
          ]
        },
        "id": "eIQdg2fI40Uz",
        "outputId": "70eae311-0b66-4c95-e293-9b1ecd01b16d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Preparing Wikipedia dataset...\n",
            "Loading Wikipedia dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/datasets/load.py:1491: FutureWarning: The repository for wikipedia contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/wikipedia\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading dataset shards:   0%|          | 0/41 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a36c5f265b5442648ddee159b7e5e887"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import gc\n",
        "from typing import List, Tuple\n",
        "from itertools import islice\n",
        "from transformers import GPT2Tokenizer, GPT2TokenizerFast\n",
        "from accelerate import Accelerator\n",
        "\n",
        "class WikiTextDataset(Dataset):\n",
        "    def __init__(self, encodings, block_size):\n",
        "        self.encodings = encodings\n",
        "        self.block_size = block_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings) - self.block_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        chunk = self.encodings[idx:idx + self.block_size + 1]\n",
        "        x = chunk[:-1]\n",
        "        y = chunk[1:]\n",
        "        return x, y\n",
        "\n",
        "def batch_iterator(iterable, batch_size):\n",
        "    \"\"\"Helper function to create batch iterator\"\"\"\n",
        "    iterator = iter(iterable)\n",
        "    while batch := list(islice(iterator, batch_size)):\n",
        "        yield batch\n",
        "\n",
        "def process_batch_gpu(\n",
        "    texts: List[str],\n",
        "    tokenizer: GPT2TokenizerFast,\n",
        "    device: torch.device,\n",
        "    max_length: int = None\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"Process a batch of texts using GPU-accelerated tokenizer\"\"\"\n",
        "    # Tokenize all texts in the batch\n",
        "    encoded = tokenizer(\n",
        "        texts,\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        return_tensors='pt',\n",
        "        return_attention_mask=True\n",
        "    ).to(device)\n",
        "\n",
        "    # Get input ids and convert to list\n",
        "    tokens = encoded.input_ids\n",
        "    attention_mask = encoded.attention_mask\n",
        "\n",
        "    # Remove padding and flatten\n",
        "    tokens_list = []\n",
        "    for seq, mask in zip(tokens, attention_mask):\n",
        "        # Only keep non-padding tokens\n",
        "        valid_tokens = seq[mask.bool()]\n",
        "        if len(valid_tokens) > 0:\n",
        "            tokens_list.append(valid_tokens)\n",
        "\n",
        "    # Concatenate all tokens\n",
        "    if tokens_list:\n",
        "        return torch.cat(tokens_list)\n",
        "    return torch.tensor([], device=device)\n",
        "\n",
        "def prepare_wikipedia_data(\n",
        "    cache_dir=\"wiki_cache\",\n",
        "    block_size=1024,\n",
        "    train_val_split=0.95,\n",
        "    batch_size=32,\n",
        "    processing_batch_size=1000,\n",
        "    device='cuda' if torch.cuda.is_available() else 'cpu'\n",
        "):\n",
        "    \"\"\"\n",
        "    Prepare Wikipedia dataset for GPT-2 training with GPU acceleration\n",
        "    \"\"\"\n",
        "    # Create cache directory if it doesn't exist\n",
        "    os.makedirs(cache_dir, exist_ok=True)\n",
        "\n",
        "    # Cache files\n",
        "    train_cache = os.path.join(cache_dir, \"train_tokens.pt\")\n",
        "    val_cache = os.path.join(cache_dir, \"val_tokens.pt\")\n",
        "\n",
        "    # Check if processed data already exists\n",
        "    if os.path.exists(train_cache) and os.path.exists(val_cache):\n",
        "        print(\"Loading cached data...\")\n",
        "        train_data = torch.load(train_cache)\n",
        "        val_data = torch.load(val_cache)\n",
        "        return train_data, val_data\n",
        "\n",
        "    print(\"Loading Wikipedia dataset...\")\n",
        "    dataset = load_dataset(\"wikipedia\", \"20220301.en\", split=\"train\")\n",
        "\n",
        "    # Initialize tokenizer with GPU acceleration\n",
        "    print(\"Initializing GPU-accelerated tokenizer...\")\n",
        "    tokenizer = GPT2TokenizerFast.from_pretrained(\n",
        "        'gpt2',\n",
        "        model_max_length=block_size\n",
        "    )\n",
        "\n",
        "    # Add special tokens if needed\n",
        "    special_tokens = {\n",
        "        'pad_token': '<|pad|>',\n",
        "        'eos_token': '<|endoftext|>'\n",
        "    }\n",
        "    tokenizer.add_special_tokens(special_tokens)\n",
        "\n",
        "    print(\"Processing dataset in batches...\")\n",
        "    all_tokens = []\n",
        "    total_batches = len(dataset) // processing_batch_size + (1 if len(dataset) % processing_batch_size != 0 else 0)\n",
        "\n",
        "    try:\n",
        "        for batch_idx, batch in enumerate(tqdm(\n",
        "            batch_iterator(dataset['text'], processing_batch_size),\n",
        "            total=total_batches,\n",
        "            desc=\"Processing batches\"\n",
        "        )):\n",
        "            # Filter out empty strings and very short texts\n",
        "            batch = [text for text in batch if len(text.strip()) > 50]\n",
        "\n",
        "            if not batch:\n",
        "                continue\n",
        "\n",
        "            # Process batch using GPU\n",
        "            batch_tokens = process_batch_gpu(batch, tokenizer, device)\n",
        "\n",
        "            if len(batch_tokens) > 0:\n",
        "                # Move to CPU to save GPU memory\n",
        "                all_tokens.append(batch_tokens.cpu())\n",
        "\n",
        "            # Periodically clear GPU memory\n",
        "            if (batch_idx + 1) % 10 == 0:\n",
        "                torch.cuda.empty_cache()\n",
        "                gc.collect()\n",
        "\n",
        "            # Periodically save progress\n",
        "            if (batch_idx + 1) % 100 == 0:\n",
        "                print(f\"\\nProcessed {batch_idx + 1}/{total_batches} batches\")\n",
        "                print(f\"Current total tokens: {sum(len(t) for t in all_tokens):,}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during processing: {str(e)}\")\n",
        "        # Save what we have so far\n",
        "        print(\"Saving partial progress...\")\n",
        "\n",
        "    finally:\n",
        "        # Concatenate all tokens\n",
        "        print(\"Concatenating tokens...\")\n",
        "        all_tokens = torch.cat(all_tokens)\n",
        "\n",
        "        # Split into train and validation\n",
        "        split_idx = int(len(all_tokens) * train_val_split)\n",
        "        train_tokens = all_tokens[:split_idx]\n",
        "        val_tokens = all_tokens[split_idx:]\n",
        "\n",
        "        print(f\"Total tokens: {len(all_tokens):,}\")\n",
        "        print(f\"Train tokens: {len(train_tokens):,}\")\n",
        "        print(f\"Val tokens: {len(val_tokens):,}\")\n",
        "\n",
        "        # Save processed data\n",
        "        print(\"Saving processed data...\")\n",
        "        torch.save(train_tokens, train_cache)\n",
        "        torch.save(val_tokens, val_cache)\n",
        "\n",
        "        # Clear memory\n",
        "        del all_tokens\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "        return train_tokens, val_tokens\n",
        "\n",
        "# Example usage with better error handling and GPU memory management\n",
        "def main():\n",
        "    # Set device\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Set up accelerator for distributed training if needed\n",
        "    accelerator = Accelerator()\n",
        "\n",
        "    try:\n",
        "        # Prepare data with larger batch size for GPU processing\n",
        "        train_tokens, val_tokens = prepare_wikipedia_data(\n",
        "            cache_dir=\"wiki_cache\",\n",
        "            block_size=1024,\n",
        "            train_val_split=0.95,\n",
        "            processing_batch_size=2000,  # Increased batch size for GPU\n",
        "            device=device\n",
        "        )\n",
        "\n",
        "        # Create dataloaders (rest of the code remains the same)\n",
        "        train_loader, val_loader = create_dataloaders(\n",
        "            train_tokens,\n",
        "            val_tokens,\n",
        "            block_size=1024,\n",
        "            batch_size=32\n",
        "        )\n",
        "\n",
        "        print(\"Data preparation completed successfully!\")\n",
        "        return train_loader, val_loader\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during data preparation: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "    finally:\n",
        "        # Clean up GPU memory\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "OX3iKADnTBhO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2TokenizerFast\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class TextGenerator:\n",
        "    def __init__(self, model, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
        "        self.model = model.to(device)\n",
        "        self.device = device\n",
        "        self.tokenizer = GPT2TokenizerFast.from_pretrained('gpt2')\n",
        "        if not self.tokenizer.pad_token:\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "        self.model.eval()\n",
        "\n",
        "    def generate(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        max_new_tokens: int = 100,\n",
        "        temperature: float = 0.8,\n",
        "        top_k: int = 50,\n",
        "        top_p: float = 0.9,\n",
        "        num_return_sequences: int = 1,\n",
        "    ):\n",
        "        \"\"\"Generate text based on a prompt\"\"\"\n",
        "        # Tokenize the prompt\n",
        "        encoded = self.tokenizer(prompt, return_tensors='pt', truncation=True)\n",
        "        input_ids = encoded['input_ids'].to(self.device)\n",
        "        input_ids = input_ids.repeat(num_return_sequences, 1)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for _ in range(max_new_tokens):\n",
        "                # Get model predictions\n",
        "                logits, _ = self.model(input_ids)\n",
        "                next_token_logits = logits[:, -1, :] / temperature\n",
        "\n",
        "                # Apply top-k filtering\n",
        "                if top_k > 0:\n",
        "                    indices_to_remove = next_token_logits < torch.topk(next_token_logits, top_k)[0][..., -1, None]\n",
        "                    next_token_logits[indices_to_remove] = float('-inf')\n",
        "\n",
        "                # Sample from the filtered distribution\n",
        "                probs = F.softmax(next_token_logits, dim=-1)\n",
        "                next_token = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "                # Stop if all sequences have generated EOS token\n",
        "                if (next_token == self.tokenizer.eos_token_id).all():\n",
        "                    break\n",
        "\n",
        "                input_ids = torch.cat([input_ids, next_token], dim=-1)\n",
        "\n",
        "        # Decode generated sequences\n",
        "        generated_sequences = []\n",
        "        for seq in input_ids:\n",
        "            text = self.tokenizer.decode(seq, skip_special_tokens=True)\n",
        "            generated_sequences.append(text)\n",
        "\n",
        "        return generated_sequences\n",
        "\n",
        "def generate_text(model_path, prompt):\n",
        "    \"\"\"Simple function to load model and generate text\"\"\"\n",
        "    # Load the checkpoint\n",
        "    checkpoint = torch.load(model_path)\n",
        "\n",
        "    # Initialize model with the saved config\n",
        "    model = GPT(checkpoint['config'])\n",
        "    model.load_state_dict(checkpoint['model'])\n",
        "\n",
        "    # Create generator\n",
        "    generator = TextGenerator(model)\n",
        "\n",
        "    # Generate text with different temperatures\n",
        "    print(\"\\nGenerating with different temperatures:\")\n",
        "    temperatures = [0.5, 0.8, 1.0]\n",
        "\n",
        "    for temp in temperatures:\n",
        "        print(f\"\\nTemperature: {temp}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        generated_texts = generator.generate(\n",
        "            prompt=prompt,\n",
        "            max_new_tokens=100,\n",
        "            temperature=temp,\n",
        "            num_return_sequences=1\n",
        "        )\n",
        "\n",
        "        print(generated_texts[0])\n",
        "\n",
        "    return generator  # Return generator for further use if needed\n",
        "\n",
        "# Example usage in Colab\n",
        "model_path = 'checkpoints/best_model.pt'  # Update this path\n",
        "prompt = \"Once upon a time\"\n",
        "\n",
        "# Generate text\n",
        "generator = generate_text(model_path, prompt)\n",
        "\n",
        "# For interactive use, you can now use the generator directly:\n",
        "while True:\n",
        "    user_prompt = input(\"\\nEnter prompt (or 'quit' to exit): \")\n",
        "    if user_prompt.lower() == 'quit':\n",
        "        break\n",
        "\n",
        "    temp = float(input(\"Enter temperature (0.1-2.0, default 0.8): \") or 0.8)\n",
        "\n",
        "    generated = generator.generate(\n",
        "        prompt=user_prompt,\n",
        "        temperature=temp,\n",
        "        max_new_tokens=100\n",
        "    )\n",
        "\n",
        "    print(\"\\nGenerated text:\")\n",
        "    print(\"-\" * 50)\n",
        "    print(generated[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        },
        "id": "EFBR2gw8JNLx",
        "outputId": "141340fa-dae5-42d4-e409-2f3fb60506b1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-3f25e1fb0f68>:61: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(model_path)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'checkpoints/best_model.pt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-3f25e1fb0f68>\u001b[0m in \u001b[0;36m<cell line: 94>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;31m# Generate text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m \u001b[0mgenerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;31m# For interactive use, you can now use the generator directly:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-3f25e1fb0f68>\u001b[0m in \u001b[0;36mgenerate_text\u001b[0;34m(model_path, prompt)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;34m\"\"\"Simple function to load model and generate text\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;31m# Load the checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;31m# Initialize model with the saved config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1317\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1319\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1320\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 659\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"w\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 640\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'checkpoints/best_model.pt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q_RSgvBQTJ_F"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}