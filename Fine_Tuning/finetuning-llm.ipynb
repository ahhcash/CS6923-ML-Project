{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nfrom datasets import load_dataset, concatenate_datasets\nfrom transformers import (\n    GPT2LMHeadModel,\n    GPT2Tokenizer,\n    DataCollatorForLanguageModeling,\n    Trainer,\n    TrainingArguments\n)\nimport os\nfrom tqdm import tqdm\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\nif torch.cuda.is_available():\n    print(f\"GPU Model: {torch.cuda.get_device_name(0)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T16:15:15.867558Z","iopub.execute_input":"2024-11-30T16:15:15.868013Z","iopub.status.idle":"2024-11-30T16:15:35.379367Z","shell.execute_reply.started":"2024-11-30T16:15:15.867979Z","shell.execute_reply":"2024-11-30T16:15:35.377979Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nGPU Model: Tesla P100-PCIE-16GB\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"os.environ[\"WANDB_DISABLED\"] = \"true\"\nmodel_name = \"gpt2\"\nprint(f\"Loading {model_name} model and tokenizer...\")\ntokenizer = GPT2Tokenizer.from_pretrained(model_name)\nmodel = GPT2LMHeadModel.from_pretrained(model_name)\n\ntokenizer.pad_token = tokenizer.eos_token\nmodel.config.pad_token_id = model.config.eos_token_id\nmodel = model.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T16:18:56.882467Z","iopub.execute_input":"2024-11-30T16:18:56.883213Z","iopub.status.idle":"2024-11-30T16:19:03.976119Z","shell.execute_reply.started":"2024-11-30T16:18:56.883179Z","shell.execute_reply":"2024-11-30T16:19:03.975104Z"}},"outputs":[{"name":"stdout","text":"Loading gpt2 model and tokenizer...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a1025f1d30fb4b989587ca11547ea48c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"36bdfecafef44121ad61eb38c642fcf8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"760bc48ebc2a43ba9b3440678d1fb84d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff4d0cbe4dd14b59a60a2e0ee042dde0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7ae7e1cd1444dcbbcfa5ef1ea939cd4"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9298a497231c430f944f6c75ba32ab4f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c293a466243f43a9b3a588697a8b37b5"}},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"def load_and_tokenize_datasets(debug_mode=True):\n\n    print(\"Loading CNN/DailyMail dataset...\")\n    summarization = load_dataset(\"giuliadc/cnndm-filtered\")\n    \n    print(\"Loading SQuAD dataset...\")\n    qa = load_dataset(\"squad\")\n    \n    if debug_mode:\n        print(\"Debug mode: Using small subset of data\")\n        summarization = {\n            'train': summarization['train'].select(range(100)),\n            'validation': summarization['validation'].select(range(20))\n        }\n        qa = {\n            'train': qa['train'].select(range(100)),\n            'validation': qa['validation'].select(range(20))\n        }\n    \n  \n    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n    tokenizer.pad_token = tokenizer.eos_token\n    \n\n    def prepare_summarization(examples):\n        texts = [f\"Article: {article}\\nSummary: {summary}\" \n                for article, summary in zip(examples['article'], examples['highlights'])]\n        return tokenizer(texts, truncation=True, max_length=512, padding=\"max_length\")\n    \n   \n    def prepare_qa(examples):\n        texts = [f\"Question: {question}\\nContext: {context}\\nAnswer: {answer['text'][0]}\"\n                for question, context, answer in zip(examples['question'], examples['context'], examples['answers'])]\n        return tokenizer(texts, truncation=True, max_length=512, padding=\"max_length\")\n    \n    print(\"Processing datasets...\")\n    tokenized_summ = summarization['train'].map(\n        prepare_summarization,\n        remove_columns=summarization['train'].column_names,\n        batched=True\n    )\n    \n    tokenized_qa = qa['train'].map(\n        prepare_qa,\n        remove_columns=qa['train'].column_names,\n        batched=True\n    )\n    \n   \n    combined_dataset = concatenate_datasets([tokenized_summ, tokenized_qa])\n    \n    print(f\"Total examples: {len(combined_dataset)}\")\n    return combined_dataset, tokenizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T16:27:51.833709Z","iopub.execute_input":"2024-11-30T16:27:51.834078Z","iopub.status.idle":"2024-11-30T16:27:51.842438Z","shell.execute_reply.started":"2024-11-30T16:27:51.834045Z","shell.execute_reply":"2024-11-30T16:27:51.841587Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"combined_dataset, tokenizer = load_and_tokenize_datasets(debug_mode=True)  \nmodel = GPT2LMHeadModel.from_pretrained('gpt2')\nmodel.config.pad_token_id = model.config.eos_token_id\nmodel = model.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T16:28:19.040921Z","iopub.execute_input":"2024-11-30T16:28:19.041314Z","iopub.status.idle":"2024-11-30T16:28:29.694675Z","shell.execute_reply.started":"2024-11-30T16:28:19.041282Z","shell.execute_reply":"2024-11-30T16:28:29.693849Z"}},"outputs":[{"name":"stdout","text":"Loading CNN/DailyMail dataset...\nLoading SQuAD dataset...\nDebug mode: Using small subset of data\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Processing datasets...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/100 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42a8cca325cd4c9589c8ad05749425f2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/100 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3847dfa53d1f4beea2d5c146fd011a3d"}},"metadata":{}},{"name":"stdout","text":"Total examples: 200\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=\"./gpt2_finetuned\",\n    num_train_epochs=1,\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=4,\n    learning_rate=5e-5,\n    fp16=True,\n    logging_steps=100,\n    save_strategy=\"epoch\",\n    logging_dir=\"./logs\",\n    report_to=\"none\"  \n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=combined_dataset,\n    data_collator=DataCollatorForLanguageModeling(\n        tokenizer=tokenizer,\n        mlm=False\n    )\n)\n\nprint(\"\\nStarting training...\")\ntrainer.train()\n\nprint(\"\\nSaving model...\")\ntrainer.save_model(\"./final_model\")\ntokenizer.save_pretrained(\"./final_model\")\nprint(\"\\nTraining completed successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T16:28:50.043936Z","iopub.execute_input":"2024-11-30T16:28:50.044807Z","iopub.status.idle":"2024-11-30T16:29:15.594686Z","shell.execute_reply.started":"2024-11-30T16:28:50.044756Z","shell.execute_reply":"2024-11-30T16:29:15.593819Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n","output_type":"stream"},{"name":"stdout","text":"\nStarting training...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [12/12 00:21, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"\nSaving model...\n\nTraining completed successfully!\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\ndef evaluate_model(model, eval_dataset, tokenizer):\n    eval_args = TrainingArguments(\n        output_dir=\"./eval_results\",\n        do_train=False,\n        do_eval=True,\n        per_device_eval_batch_size=8,\n        report_to=\"none\"\n    )\n    \n    evaluator = Trainer(\n        model=model,\n        args=eval_args,\n        eval_dataset=eval_dataset,\n        data_collator=DataCollatorForLanguageModeling(\n            tokenizer=tokenizer,\n            mlm=False\n        )\n    )\n    \n    print(\"Running evaluation...\")\n    metrics = evaluator.evaluate()\n    \n    perplexity = np.exp(metrics['eval_loss'])\n    cross_entropy = metrics['eval_loss']\n    \n    print(\"\\nEvaluation Results:\")\n    print(f\"Perplexity: {perplexity:.2f}\")\n    print(f\"Cross Entropy Loss: {cross_entropy:.4f}\")\n    \n    return perplexity, cross_entropy\n\nprint(\"Evaluating fine-tuned model...\")\nperplexity, cross_entropy = evaluate_model(model, combined_dataset, tokenizer)\n\nresults = {\n    'Model': ['GPT (Fine-tuned)'],\n    'Perplexity': [perplexity],\n    'Cross Entropy Loss': [cross_entropy]\n}\n\ncomparison_df = pd.DataFrame(results)\nprint(\"\\nModel Comparison Results:\")\nprint(comparison_df)\n\ncomparison_df.to_csv('model_comparison_results.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T16:34:22.063532Z","iopub.execute_input":"2024-11-30T16:34:22.064276Z","iopub.status.idle":"2024-11-30T16:34:27.246478Z","shell.execute_reply.started":"2024-11-30T16:34:22.064241Z","shell.execute_reply":"2024-11-30T16:34:27.245623Z"}},"outputs":[{"name":"stdout","text":"Evaluating fine-tuned model...\nRunning evaluation...\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [25/25 00:04]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"\nEvaluation Results:\nPerplexity: 18.48\nCross Entropy Loss: 2.9166\n\nModel Comparison Results:\n              Model  Perplexity  Cross Entropy Loss\n0  GPT (Fine-tuned)   18.477742            2.916567\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}