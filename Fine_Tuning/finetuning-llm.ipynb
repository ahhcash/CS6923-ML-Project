{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nfrom datasets import load_dataset, concatenate_datasets\nfrom transformers import (\n    GPT2LMHeadModel,\n    GPT2Tokenizer,\n    DataCollatorForLanguageModeling,\n    Trainer,\n    TrainingArguments\n)\nimport os\nfrom tqdm import tqdm\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\nif torch.cuda.is_available():\n    print(f\"GPU Model: {torch.cuda.get_device_name(0)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T19:07:50.948395Z","iopub.execute_input":"2024-12-08T19:07:50.949725Z","iopub.status.idle":"2024-12-08T19:07:50.955585Z","shell.execute_reply.started":"2024-12-08T19:07:50.949688Z","shell.execute_reply":"2024-12-08T19:07:50.954684Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nGPU Model: Tesla P100-PCIE-16GB\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"os.environ[\"WANDB_DISABLED\"] = \"true\"\nprint(os.path.exists(\"./final_model\"))\nmodel_name = \"gpt2\"\nprint(f\"Loading {model_name} model and tokenizer...\")\ntokenizer = GPT2Tokenizer.from_pretrained(model_name)\nmodel = GPT2LMHeadModel.from_pretrained(model_name)\n\ntokenizer.pad_token = tokenizer.eos_token\nmodel.config.pad_token_id = model.config.eos_token_id\nmodel = model.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T19:07:53.372249Z","iopub.execute_input":"2024-12-08T19:07:53.372599Z","iopub.status.idle":"2024-12-08T19:07:54.360567Z","shell.execute_reply.started":"2024-12-08T19:07:53.372569Z","shell.execute_reply":"2024-12-08T19:07:54.359830Z"}},"outputs":[{"name":"stdout","text":"True\nLoading gpt2 model and tokenizer...\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"def load_and_tokenize_datasets(debug_mode=True):\n\n    print(\"Loading CNN/DailyMail dataset...\")\n    summarization = load_dataset(\"giuliadc/cnndm-filtered\")\n    \n    print(\"Loading SQuAD dataset...\")\n    qa = load_dataset(\"squad\")\n    \n    if debug_mode:\n        print(\"Debug mode: Using small subset of data\")\n        summarization = {\n            'train': summarization['train'].select(range(100)),\n            'validation': summarization['validation'].select(range(20))\n        }\n        qa = {\n            'train': qa['train'].select(range(100)),\n            'validation': qa['validation'].select(range(20))\n        }\n    \n  \n    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n    tokenizer.pad_token = tokenizer.eos_token\n    \n\n    def prepare_summarization(examples):\n        texts = [f\"Article: {article}\\nSummary: {summary}\" \n                for article, summary in zip(examples['article'], examples['highlights'])]\n        return tokenizer(texts, truncation=True, max_length=512, padding=\"max_length\")\n    \n   \n    def prepare_qa(examples):\n        texts = [f\"Question: {question}\\nContext: {context}\\nAnswer: {answer['text'][0]}\"\n                for question, context, answer in zip(examples['question'], examples['context'], examples['answers'])]\n        return tokenizer(texts, truncation=True, max_length=512, padding=\"max_length\")\n    \n    print(\"Processing datasets...\")\n    tokenized_summ = summarization['train'].map(\n        prepare_summarization,\n        remove_columns=summarization['train'].column_names,\n        batched=True\n    )\n    \n    tokenized_qa = qa['train'].map(\n        prepare_qa,\n        remove_columns=qa['train'].column_names,\n        batched=True\n    )\n    \n   \n    combined_dataset = concatenate_datasets([tokenized_summ, tokenized_qa])\n    \n    print(f\"Total examples: {len(combined_dataset)}\")\n    return combined_dataset, tokenizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T19:07:58.422690Z","iopub.execute_input":"2024-12-08T19:07:58.423492Z","iopub.status.idle":"2024-12-08T19:07:58.431486Z","shell.execute_reply.started":"2024-12-08T19:07:58.423460Z","shell.execute_reply":"2024-12-08T19:07:58.430571Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"combined_dataset, tokenizer = load_and_tokenize_datasets(debug_mode=True)  \nmodel = GPT2LMHeadModel.from_pretrained('gpt2')\nmodel.config.pad_token_id = model.config.eos_token_id\nmodel = model.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T19:08:04.260819Z","iopub.execute_input":"2024-12-08T19:08:04.261477Z","iopub.status.idle":"2024-12-08T19:08:11.234102Z","shell.execute_reply.started":"2024-12-08T19:08:04.261445Z","shell.execute_reply":"2024-12-08T19:08:11.233188Z"}},"outputs":[{"name":"stdout","text":"Loading CNN/DailyMail dataset...\nLoading SQuAD dataset...\nDebug mode: Using small subset of data\nProcessing datasets...\nTotal examples: 200\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=\"./gpt2_finetuned\",\n    num_train_epochs=1,\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=4,\n    learning_rate=5e-5,\n    fp16=True,\n    logging_steps=100,\n    save_strategy=\"epoch\",\n    logging_dir=\"./logs\",\n    report_to=\"none\"  \n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=combined_dataset,\n    data_collator=DataCollatorForLanguageModeling(\n        tokenizer=tokenizer,\n        mlm=False\n    )\n)\n\nprint(\"\\nStarting training...\")\ntrainer.train()\n\nprint(\"\\nSaving model...\")\ntrainer.save_model(\"./final_model\")\ntokenizer.save_pretrained(\"./final_model\")\nprint(\"\\nTraining completed successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T19:08:13.828125Z","iopub.execute_input":"2024-12-08T19:08:13.828827Z","iopub.status.idle":"2024-12-08T19:08:40.950586Z","shell.execute_reply.started":"2024-12-08T19:08:13.828793Z","shell.execute_reply":"2024-12-08T19:08:40.949696Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n","output_type":"stream"},{"name":"stdout","text":"\nStarting training...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [12/12 00:22, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"\nSaving model...\n\nTraining completed successfully!\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\ndef evaluate_model(model, eval_dataset, tokenizer):\n    eval_args = TrainingArguments(\n        output_dir=\"./eval_results\",\n        do_train=False,\n        do_eval=True,\n        per_device_eval_batch_size=8,\n        report_to=\"none\"\n    )\n    \n    evaluator = Trainer(\n        model=model,\n        args=eval_args,\n        eval_dataset=eval_dataset,\n        data_collator=DataCollatorForLanguageModeling(\n            tokenizer=tokenizer,\n            mlm=False\n        )\n    )\n    \n    print(\"Running evaluation...\")\n    metrics = evaluator.evaluate()\n    \n    perplexity = np.exp(metrics['eval_loss'])\n    cross_entropy = metrics['eval_loss']\n    \n    print(\"\\nEvaluation Results:\")\n    print(f\"Perplexity: {perplexity:.2f}\")\n    print(f\"Cross Entropy Loss: {cross_entropy:.4f}\")\n    \n    return perplexity, cross_entropy\n\nprint(\"Evaluating fine-tuned model...\")\nperplexity, cross_entropy = evaluate_model(model, combined_dataset, tokenizer)\n\nresults = {\n    'Model': ['GPT (Fine-tuned)'],\n    'Perplexity': [perplexity],\n    'Cross Entropy Loss': [cross_entropy]\n}\n\ncomparison_df = pd.DataFrame(results)\nprint(\"\\nModel Comparison Results:\")\nprint(comparison_df)\n\ncomparison_df.to_csv('model_comparison_results.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T19:08:44.017620Z","iopub.execute_input":"2024-12-08T19:08:44.018325Z","iopub.status.idle":"2024-12-08T19:08:49.193816Z","shell.execute_reply.started":"2024-12-08T19:08:44.018291Z","shell.execute_reply":"2024-12-08T19:08:49.192856Z"}},"outputs":[{"name":"stdout","text":"Evaluating fine-tuned model...\nRunning evaluation...\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [25/25 00:04]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"\nEvaluation Results:\nPerplexity: 18.48\nCross Entropy Loss: 2.9166\n\nModel Comparison Results:\n              Model  Perplexity  Cross Entropy Loss\n0  GPT (Fine-tuned)   18.477742            2.916567\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"from transformers import GPT2LMHeadModel, GPT2Tokenizer\nimport torch\n\ndef load_fine_tuned_model():\n    model_path = \"./final_model\"\n    tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n    model = GPT2LMHeadModel.from_pretrained(model_path)\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n    tokenizer.pad_token = tokenizer.eos_token\n    model.config.pad_token_id = model.config.eos_token_id\n    \n    model = model.to(device)\n    return model, tokenizer\n\n# Test Articles for Summarization\ntest_articles = [\n    # Technology\n    {\n        \"title\": \"AI Breakthrough\",\n        \"text\": \"\"\"OpenAI researchers have announced a major breakthrough in artificial intelligence. \n        The new model demonstrates unprecedented capabilities in understanding and generating \n        human language. In extensive testing, it showed remarkable ability to write code, \n        solve complex mathematical problems, and engage in nuanced dialogue. However, \n        researchers emphasize the importance of responsible AI development and have \n        implemented robust safety measures. The technology could revolutionize fields \n        from education to scientific research.\"\"\"\n    },\n    \n    # Science\n    {\n        \"title\": \"Climate Research\",\n        \"text\": \"\"\"A groundbreaking study published in Nature reveals alarming rates of Arctic ice \n        melting. Scientists have found that the rate of melting has doubled in the past \n        decade, far exceeding previous predictions. The research, conducted over five \n        years, combined satellite data with on-site measurements. If current trends \n        continue, sea levels could rise by up to two meters by 2100, threatening \n        coastal cities worldwide. The study calls for immediate action to reduce \n        greenhouse gas emissions.\"\"\"\n    },\n    \n    # Healthcare\n    {\n        \"title\": \"Medical Discovery\",\n        \"text\": \"\"\"Researchers at Stanford Medical Center have developed a new cancer treatment \n        that shows promising results. The therapy combines traditional immunotherapy \n        with targeted drug delivery, effectively reducing tumor size in 85% of trial \n        participants. Side effects were minimal compared to conventional treatments. \n        The breakthrough could particularly benefit patients with aggressive forms \n        of breast and lung cancer. Clinical trials are expected to expand to more \n        hospitals next year.\"\"\"\n    }\n]\n\n# Test QA Pairs\ntest_qa_sets = [\n    {\n        \"context\": \"\"\"The Internet was developed in the 1960s by the United States Department of \n        Defense through its ARPANET project. Initially designed as a military \n        communication network that could survive a nuclear attack, it evolved into \n        the modern Internet by the 1990s. Tim Berners-Lee later invented the World \n        Wide Web in 1989 while working at CERN, making the Internet more accessible \n        to the general public.\"\"\",\n        \"questions\": [\n            \"When was the Internet developed?\",\n            \"What was the original purpose of ARPANET?\",\n            \"Who invented the World Wide Web?\",\n            \"Where was the World Wide Web invented?\"\n        ]\n    },\n    {\n        \"context\": \"\"\"Quantum computing leverages quantum mechanical phenomena like superposition \n        and entanglement to perform computations. Unlike classical computers that \n        use bits (0 or 1), quantum computers use quantum bits or qubits that can \n        exist in multiple states simultaneously. This property could potentially \n        solve certain problems exponentially faster than classical computers, \n        particularly in areas like cryptography and molecular simulation.\"\"\",\n        \"questions\": [\n            \"What is quantum computing based on?\",\n            \"How are quantum computers different from classical computers?\",\n            \"What are the potential advantages of quantum computers?\",\n            \"What type of problems could quantum computers solve better?\"\n        ]\n    },\n    {\n        \"context\": \"\"\"Electric vehicles (EVs) have seen rapid advancement in recent years. Modern \n        EVs can travel over 300 miles on a single charge, with some models reaching \n        400+ miles. Charging technology has also improved, with fast-charging stations \n        capable of providing 200 miles of range in just 15 minutes. The cost of EV \n        batteries has dropped by 90% since 2010, making electric vehicles increasingly \n        affordable for average consumers.\"\"\",\n        \"questions\": [\n            \"What is the typical range of modern EVs?\",\n            \"How long does fast-charging take?\",\n            \"How much have EV battery costs changed?\",\n            \"What improvements have been made in EV technology?\"\n        ]\n    }\n]\n\ndef generate_text(model, tokenizer, prompt, max_new_tokens=100):\n    try:\n        inputs = tokenizer(\n            prompt,\n            return_tensors=\"pt\",\n            max_length=512,\n            truncation=True,\n            padding=True,\n            add_special_tokens=True\n        ).to(model.device)\n        \n        outputs = model.generate(\n            input_ids=inputs['input_ids'],\n            attention_mask=inputs['attention_mask'],\n            max_new_tokens=max_new_tokens,\n            num_return_sequences=1,\n            temperature=0.7,\n            top_p=0.9,\n            no_repeat_ngram_size=3,\n            do_sample=True,\n            early_stopping=True,\n            pad_token_id=tokenizer.eos_token_id\n        )\n        \n        return tokenizer.decode(outputs[0], skip_special_tokens=True)\n    except Exception as e:\n        print(f\"Error generating text: {str(e)}\")\n        return \"\"\n\ndef test_model():\n    try:\n        print(\"Loading fine-tuned model...\")\n        model, tokenizer = load_fine_tuned_model()\n        \n        print(\"\\n=== Testing Summarization Capabilities ===\")\n        for article in test_articles:\n            print(f\"\\nArticle Topic: {article['title']}\")\n            print(\"Original Text:\")\n            print(article['text'].strip())\n            \n            prompt = f\"Summarize this article:\\n{article['text']}\\nSummary:\"\n            generated_text = generate_text(model, tokenizer, prompt, max_new_tokens=150)\n            \n            print(\"\\nGenerated Summary:\")\n            summary = generated_text.split(\"Summary:\")[-1].strip() if \"Summary:\" in generated_text else generated_text\n            print(summary)\n            print(\"\\n\" + \"=\"*80)\n        \n        print(\"\\n=== Testing Question Answering Capabilities ===\")\n        for i, qa_set in enumerate(test_qa_sets, 1):\n            print(f\"\\nTest Set {i}:\")\n            print(\"Context:\", qa_set['context'].strip())\n            \n            for question in qa_set['questions']:\n                prompt = f\"Based on the context, answer this question:\\nContext: {qa_set['context']}\\nQuestion: {question}\\nAnswer:\"\n                generated_text = generate_text(model, tokenizer, prompt, max_new_tokens=100)\n                \n                print(f\"\\nQuestion: {question}\")\n                answer = generated_text.split(\"Answer:\")[-1].strip() if \"Answer:\" in generated_text else generated_text\n                print(\"Generated Answer:\", answer)\n            print(\"\\n\" + \"=\"*80)\n            \n    except Exception as e:\n        print(f\"Error during testing: {str(e)}\")\n\nif __name__ == \"__main__\":\n    test_model()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T19:09:15.581886Z","iopub.execute_input":"2024-12-08T19:09:15.582230Z","iopub.status.idle":"2024-12-08T19:09:30.848905Z","shell.execute_reply.started":"2024-12-08T19:09:15.582198Z","shell.execute_reply":"2024-12-08T19:09:30.848051Z"}},"outputs":[{"name":"stdout","text":"Loading fine-tuned model...\n\n=== Testing Summarization Capabilities ===\n\nArticle Topic: AI Breakthrough\nOriginal Text:\nOpenAI researchers have announced a major breakthrough in artificial intelligence. \n        The new model demonstrates unprecedented capabilities in understanding and generating \n        human language. In extensive testing, it showed remarkable ability to write code, \n        solve complex mathematical problems, and engage in nuanced dialogue. However, \n        researchers emphasize the importance of responsible AI development and have \n        implemented robust safety measures. The technology could revolutionize fields \n        from education to scientific research.\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:649: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\nGenerated Summary:\nOpen AI researchers have revealed a breakthrough in Artificial Intelligence, making it possible to build intelligent machines that can learn from experience and learn from their mistakes.     \nThe new model shows unprecedented capabilities and has an enormous impact on fields from education and medicine to health and the environment.  The model shows remarkable ability \nto write code , solve complex problems, engage in complex dialogue , and engage with nuanced dialogue .  It can create complex problems , learn from the experience of its environment , and build complex models that can quickly identify, diagnose, and deal with problems in the environment \nIt can generate complex models of complex problems and generate complex solutions to complex problems . \nIts predictive modeling and predictive analytics capabilities are\n\n================================================================================\n\nArticle Topic: Climate Research\nOriginal Text:\nA groundbreaking study published in Nature reveals alarming rates of Arctic ice \n        melting. Scientists have found that the rate of melting has doubled in the past \n        decade, far exceeding previous predictions. The research, conducted over five \n        years, combined satellite data with on-site measurements. If current trends \n        continue, sea levels could rise by up to two meters by 2100, threatening \n        coastal cities worldwide. The study calls for immediate action to reduce \n        greenhouse gas emissions.\n\nGenerated Summary:\nSea levels have risen by up more than 2 meters over the past decade, making it the largest \n(5,000-foot) increase in the world's recorded history. The new study   has led to urgent action \n to reduce greenhouse gas emission and to avoid any further mass extinctions \nIn a paper published in the journal Nature Climate Change , researchers from the University of Washington and the University at Albany have identified a new, faster-growing \nof ice that's already melting.   The new ice was found to be a 2-meter (4-foot-wide) ice core that was 3 meters (6 feet) thick and had a thickness of 1,000 kilometers (about 5 miles) thick\n\n================================================================================\n\nArticle Topic: Medical Discovery\nOriginal Text:\nResearchers at Stanford Medical Center have developed a new cancer treatment \n        that shows promising results. The therapy combines traditional immunotherapy \n        with targeted drug delivery, effectively reducing tumor size in 85% of trial \n        participants. Side effects were minimal compared to conventional treatments. \n        The breakthrough could particularly benefit patients with aggressive forms \n        of breast and lung cancer. Clinical trials are expected to expand to more \n        hospitals next year.\n\nGenerated Summary:\nThe study by Stanford Medical center's Dr. Steven T. Hernagel and colleagues \n\" demonstrates a new drug for treating cancer with targeted immunotherapy and a new approach to treating tumors. This novel approach allows patients to treat tumor size reduction with the help of targeted immunotherapeutic \n\n  \nIn the latest study, the researchers found that patients treated with a combination of immunotherapy, chemotherapy and radiation therapy were able to reduce tumor size    \n\"            \n\n================================================================================\n\n=== Testing Question Answering Capabilities ===\n\nTest Set 1:\nContext: The Internet was developed in the 1960s by the United States Department of \n        Defense through its ARPANET project. Initially designed as a military \n        communication network that could survive a nuclear attack, it evolved into \n        the modern Internet by the 1990s. Tim Berners-Lee later invented the World \n        Wide Web in 1989 while working at CERN, making the Internet more accessible \n        to the general public.\n\nQuestion: When was the Internet developed?\nGenerated Answer: in the mid-1960s. The Internet began to be developed and developed into the Internet and Internet Service Provider. The first major breakthrough was the invention of the Web, which made it possible to transmit information through a network without the use of a computer. The Web was introduced in 1973.\nIn 1978, the Internet was introduced as a technology for making video, music and other media, and in 1979 it was implemented into the International Network of Information Systems. The World Wide Web was launched in 1986\n\nQuestion: What was the original purpose of ARPANET?\nGenerated Answer: To provide a communications system for the United Nations, the United Kingdom, and the United Nation.  \ncontext: The project was set up in 1957 by the U.S. Department of Defense. The Department of the Navy, the Army, and Department of Commerce were involved \n in developing the Internet as a means of communication and the military was involved   in the development of the Internet . \nThe U.N. began using ARPANAET in 1973, with\n\nQuestion: Who invented the World Wide Web?\nGenerated Answer: The United States military used the Internet as a means to develop the military   technology necessary to protect against nuclear attacks. The United \nAmes National Liberation Army (NLA) also developed the Internet. The NLA was the first military organization to use the Internet to provide direct military         \nThe United States Army, the United Kingdom, the U.S. Navy, the Army Air\n\nQuestion: Where was the World Wide Web invented?\nGenerated Answer: The World Wide Internet was first created in 1960 by the U.S. Department of Defense in response to the Soviet invasion of the Soviet Union. \nThe Pentagon developed the WorldWide Web in part to address the needs of its military   forces. In the late 1960s, the Department of Energy began testing the first web-based Internet . \nIn 1967, the National Defense University of North Carolina developed the Internet Research Institute (ENSRI), a program to develop computer and telecommunication\n\n================================================================================\n\nTest Set 2:\nContext: Quantum computing leverages quantum mechanical phenomena like superposition \n        and entanglement to perform computations. Unlike classical computers that \n        use bits (0 or 1), quantum computers use quantum bits or qubits that can \n        exist in multiple states simultaneously. This property could potentially \n        solve certain problems exponentially faster than classical computers, \n        particularly in areas like cryptography and molecular simulation.\n\nQuestion: What is quantum computing based on?\nGenerated Answer: Quantum computation relies on quantum mechanical forces. This force is called the \"quantum force\" and is caused by the presence of the quantum \"qubits\" that are attached to the qubits. In quantum computers, qubits are simply bits of data and thus do not have any physical interaction with each other. In classical computers , qubits (or \"qubit pairs\") are simply a set of data (called \"quarks\") which are attached in the form of qubits, such as a\n\nQuestion: How are quantum computers different from classical computers?\nGenerated Answer: The classical computer was a quantum computer that was based on a single photon. However, as quantum computers evolved, quantum computers became more powerful and more complex. The most famous example of this is the Quantum Computer.\nQCD Quantum Computer\nQ: Why is Quantum Computer such a popular choice for research?\nQ. The QCD Quantum computer is a quantum computing system that is based on qubits (or qubits with an embedded quantum core). The QCC is an interdisciplinary team of scientists\n\nQuestion: What are the potential advantages of quantum computers?\nGenerated Answer: Quantum computing could be used to solve some of the world's problems and solve problems in a fraction of the time   or even in the same way   *  \nquestion: What do quantum computers mean for computers?   http://www.math.cornell.edu/~larry/math/qubits/\nQuestion 1: What is the quantum computing technology?  http . . .   . . Question 2: What does quantum computing mean\n\nQuestion: What type of problems could quantum computers solve better?\nGenerated Answer: Answer : Quantum computers   could  easily solve certain       e n g problems   . In particular,   y  �  �  �  � � \"   i n g   l e n   o n  s   t h e t h o n s  (   *   ?   ,   -   0 )   :\nA\n\n================================================================================\n\nTest Set 3:\nContext: Electric vehicles (EVs) have seen rapid advancement in recent years. Modern \n        EVs can travel over 300 miles on a single charge, with some models reaching \n        400+ miles. Charging technology has also improved, with fast-charging stations \n        capable of providing 200 miles of range in just 15 minutes. The cost of EV \n        batteries has dropped by 90% since 2010, making electric vehicles increasingly \n        affordable for average consumers.\n\nQuestion: What is the typical range of modern EVs?\nGenerated Answer: The average range of EV vehicles is 1,000 miles.\nSource:  The National Institute of Transportation  (NIOT)  reported that the average range for a 1,200 mile vehicle was 3,200 miles, and the average for a 400 mile vehicle is 2,800 miles. \nSource   :  NIMOT  reports that the range of an EV is up to 3,800 kilometers, while the range for an electric vehicle is up\n\nQuestion: How long does fast-charging take?\nGenerated Answer: It takes up to 15 minutes to charge an EV, depending on the model and the vehicle.\nThe average American consumes an average of 5 hours per day of driving.\nAn average American is able to drive for 3 hours per night.\nA typical American also consumes 2 hours per morning, and 2 hours of night time.\nThis is the average American living in the United States, which is  about 8 hours per week.\nIf you are traveling in a city that has a population\n\nQuestion: How much have EV battery costs changed?\nGenerated Answer: The cost-per-mile (PPM) of a battery has been steadily increasing over the last decade. The average consumer now uses two electric vehicles a year, with the majority of EVs sold in the United States. In addition, many electric cars and motorcycles are now driven by electric powertrain manufacturers. \nThe average American consumes about 25% less fuel in a car than it does in a motorcycle, and the average American is also more likely to have a low-mileage battery than a\n\nQuestion: What improvements have been made in EV technology?\nGenerated Answer: EV technology has been steadily improving over the past five years, and the technology is still evolving. \nQuestion 1: What is the average cost of a vehicle in the United States?\nThe average US vehicle is $1,500.  \nThe US average vehicle is priced at $1 million.  We estimate the average US cost of an electric vehicle to be $6,000.\n\n================================================================================\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}